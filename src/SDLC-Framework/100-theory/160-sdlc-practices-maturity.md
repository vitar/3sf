# SDLC Practices Maturity

## Purpose

The **SDLC Practices Maturity** model describes how each of the six practices develops from isolated effort to an integrated discipline that sustains the entire SDLC system.<br/>
It provides a lens for evaluating delivery integrity and learning capability across projects, portfolios, or teams.

## Practice Maturity Levels

| **Level** | **Description** | **Behavioral Characteristics** |
|------------|----------------|--------------------------------|
| **1. Foundational Awareness** | Practice exists but is inconsistent or reactive. | Knowledge varies by person; success depends on heroics; quality unstable. |
| **2. Defined and Repeatable** | Practice has structure and shared understanding. | Common templates, predictable routines, partial feedback usage. |
| **3. Embedded and Measurable** | Practice embedded in day-to-day delivery; outcomes tracked. | Teams self-correct using data; governance observes trends, not incidents. |
| **4. Adaptive and Evolving** | Practice continuously improves through feedback and innovation. | Cross-practice learning loops; teams experiment and adjust intentionally. |

A project or team can mature unevenly — for example, strong in **Engineering & Quality (L3)** but weak in **Governance & Risk (L1)**.<br/>
The goal is not uniformity but **functional balance**.

## Maturity by Practice

| **Practice** | **Early Maturity Behavior** | **Advanced Maturity Behavior** | **Indicators of Growth** |
|---------------|-----------------------------|--------------------------------|---------------------------|
| **Product Thinking** | Focus on features and deadlines; limited validation of user value. | Focus on measurable outcomes; hypotheses validated through feedback. | Documented goals, value metrics, user feedback cycles. |
| **Architecture & Design** | Decisions implicit or reactive; tech debt grows unnoticed. | Design evolves intentionally; decisions made transparently with trade-offs visible. | Architecture review cadence, traceability of technical rationale. |
| **Engineering & Quality** | Manual testing; fragmented ownership of quality. | Quality automated and shared; engineering metrics drive improvement. | Test coverage, defect trends, build stability. |
| **DevOps & Delivery** | Manual deployments, unreliable environments. | Automated pipelines, stable environments, real-time observability. | Deployment frequency, MTTR, lead time for changes. |
| **Governance & Risk** | Bureaucratic or absent governance; unclear accountability. | Embedded governance with lightweight controls and shared responsibility. | Decision logs, compliance checks, escalation lead time. |
| **Feedback & Learning** | Retrospectives rare; insights not tracked. | Continuous feedback loops inform roadmap and process design. | Improvement actions tracked and completed, system metrics evolve. |

## Cross-Practice Synergy

Maturity in one practice reinforces others:

| **If this practice matures...** | **It strengthens...** | **By...** |
|--------------------------------|-----------------------|-----------|
| Product Thinking | Architecture & Design | Aligning solution design with user value. |
| Engineering & Quality | DevOps & Delivery | Creating reliable automation and faster feedback. |
| Governance & Risk | All others | Ensuring decisions remain transparent and aligned. |
| Feedback & Learning | The entire SDLC System | Turning insights into next-cycle improvements. |

High-performing teams reach **cross-practice coherence**, where decisions made in one discipline positively impact all others.

## When Maturity Inverts

Sometimes, a practice advances faster than its supporting ecosystem — for example, sophisticated DevOps automation in a project lacking Product Thinking or Governance clarity.<br/>
This **maturity inversion** can produce friction: faster delivery of unclear value, or automation that amplifies misaligned priorities.<br/>
3SF treats these cases as alignment gaps, not regressions — the goal is to synchronize maturity across disciplines rather than maximize any one in isolation.

## Assessing Practice Maturity

Assessment combines **qualitative feedback** (behaviors, attitudes) and **quantitative indicators** (metrics, cycle data):

- **RAC linkage:** Stable Rules (R1–R12) mapped to practice maturity signals.
- **CRC linkage:** Contextual archetypes define target maturity profiles for each practice under different project types.
- **Self-assessment cadence:** Quarterly reflection on practice evolution per project or team.

Outputs form a **Practice Maturity Radar**, helping visualize strengths, gaps, and next improvement actions.

## Summary

- SDLC Practices Maturity defines *how deeply* each discipline shapes delivery success.
- Practices evolve from awareness to adaptability, reinforcing one another.
- Balanced maturity across practices ensures the SDLC system is **self-correcting, transparent, and learning-oriented**.
- Continuous improvement of practices equals sustained organizational resilience.
